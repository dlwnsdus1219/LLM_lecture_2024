{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnistdata\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3899717.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnistdata\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnistdata\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnistdata\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 153738.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnistdata\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnistdata\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnistdata\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1404967.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnistdata\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnistdata\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnistdata\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnistdata\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnistdata\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드하기\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = MNIST(root='./mnistdata', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./mnistdata', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이건, 데이터 로더에서 또 H.P 변경이 필요한 경우를 고려하여 코드 수정!\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def create_data_loaders(batch_size):\n",
    "    transform = transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(num_classes=10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "              ReLU-2           [-1, 64, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
      "              ReLU-5          [-1, 128, 14, 14]               0\n",
      "         MaxPool2d-6            [-1, 128, 7, 7]               0\n",
      "            Conv2d-7            [-1, 256, 7, 7]         295,168\n",
      "              ReLU-8            [-1, 256, 7, 7]               0\n",
      "            Conv2d-9            [-1, 256, 7, 7]         590,080\n",
      "             ReLU-10            [-1, 256, 7, 7]               0\n",
      "        MaxPool2d-11            [-1, 256, 3, 3]               0\n",
      "           Linear-12                 [-1, 4096]       9,441,280\n",
      "             ReLU-13                 [-1, 4096]               0\n",
      "          Dropout-14                 [-1, 4096]               0\n",
      "           Linear-15                 [-1, 4096]      16,781,312\n",
      "          Dropout-16                 [-1, 4096]               0\n",
      "           Linear-17                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 27,223,306\n",
      "Trainable params: 27,223,306\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.85\n",
      "Params size (MB): 103.85\n",
      "Estimated Total Size (MB): 105.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: junylee00 (junylee00-chonnam-national-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 자, 이제 완디비를 설치해 봅시다잇\n",
    "import wandb\n",
    "\n",
    "## 개인 API 먼저!\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\jnu\\GDG on Campus CNU\\study\\wandb\\run-20250203_222311-g203psgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST/runs/g203psgk' target=\"_blank\">VGG16 Basic</a></strong> to <a href='https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST' target=\"_blank\">https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST/runs/g203psgk' target=\"_blank\">https://wandb.ai/junylee00-chonnam-national-university/Weight%20Biases%20Beginner%20with%20MNIST/runs/g203psgk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='Weight Biases Beginner with MNIST',\n",
    "           name='VGG16 Basic')\n",
    "\n",
    "wandb.watch(model)\n",
    "## 학습 그래프, GPU 시스템 등 로딩 ㄱㄱ => 하단 링크에 기록!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()     # Back Propagation\n",
    "        optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Validation 과정(with 테스트 데이터셋)\n",
    "def validate_one_epoch(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {'accuracy': 0, 'f1_score': 0}\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "             images, labels = images.to(device), labels.to(device)\n",
    "             outputs = model(images)\n",
    "             loss = criterion(outputs, labels)\n",
    "             total_loss += loss.item()\n",
    "\n",
    "             _, predicted = torch.max(outputs.data, 1)\n",
    "             total = labels.size(0)\n",
    "             correct = (predicted == labels).sum().item()\n",
    "             accuracy = correct / total\n",
    "             predicted_cpu = predicted.cpu()\n",
    "             labels_cpu = labels.cpu()\n",
    "             f1 = f1_score(labels_cpu, predicted_cpu, average='macro')\n",
    "\n",
    "             total_metrics['accuracy'] += accuracy\n",
    "             total_metrics['f1_score'] += f1\n",
    "             num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metrics = {k: v / num_batches for k, v in total_metrics.items()}\n",
    "\n",
    "    # 한 에포크마다 validation 하고 기록!!\n",
    "    wandb.log({\n",
    "        'val_accuracy': avg_metrics['accuracy'],\n",
    "        'val_f1_score': avg_metrics['f1_score']\n",
    "    })\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, train_loader, test_loader, optimizer, criterion, num_epochs=10):\n",
    "    # 에포크 마다 train, valid 고고\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss = validate_one_epoch(model, test_loader, criterion)\n",
    "\n",
    "        # 에포크 마다 train loss, val loss 기록 ㄱㄱㄱ\n",
    "        wandb.log({\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'epoch': epoch\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.0076, Val Loss: 0.0341\n",
      "Epoch 2/10, Train Loss: 0.1742, Val Loss: 0.0424\n",
      "Epoch 3/10, Train Loss: 0.0029, Val Loss: 0.0483\n",
      "Epoch 4/10, Train Loss: 0.0685, Val Loss: 0.0741\n",
      "Epoch 5/10, Train Loss: 0.0001, Val Loss: 0.0467\n",
      "Epoch 6/10, Train Loss: 0.0003, Val Loss: 0.0455\n",
      "Epoch 7/10, Train Loss: 0.0000, Val Loss: 0.0461\n",
      "Epoch 8/10, Train Loss: 0.1756, Val Loss: 0.0493\n",
      "Epoch 9/10, Train Loss: 0.0350, Val Loss: 0.0533\n",
      "Epoch 10/10, Train Loss: 0.0000, Val Loss: 0.0465\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "main(model, train_loader, test_loader, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-p6ZHjEey-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
